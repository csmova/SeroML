{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas\n",
    "import pandas as pd\n",
    "\n",
    "#scipy\n",
    "import scipy as sp, numpy as np\n",
    "import scipy.io as io\n",
    "\n",
    "#sklearn\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import scale, StandardScaler, normalize, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, MultiTaskElasticNetCV,MultiTaskElasticNet\n",
    "from sklearn.model_selection import ShuffleSplit, LeaveOneGroupOut, LeaveOneOut, train_test_split, learning_curve, GridSearchCV, cross_val_score, cross_val_predict, RepeatedKFold\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "#misc\n",
    "import warnings\n",
    "import pickle\n",
    "import mat73\n",
    "from pca import pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error handling\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_schema_from_excel(schemaFile,schemaSheet):\n",
    "    \"\"\"convert excel schema to pandas dataframe\"\"\"\n",
    "    mySchema = pd.read_excel(schemaFile, sheet_name=schemaSheet) \n",
    "    mySchema.set_index('Std ID',inplace=True)\n",
    "    return mySchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_samples(myY,to_replace,replace_with):\n",
    "    \"\"\"wrapper to help rename y data as pandas dataframe\"\"\"\n",
    "    myY.replace(to_replace,replace_with,inplace=True);\n",
    "    return myY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_concentrations(mySchema,analyteList,myXlabels,fill_null=True):\n",
    "    \"\"\"use mySchema as a lookup table for all voltammogram labels (myXlabels) for given analyteList, \n",
    "    to build and return a corresponding list of concentrations (myY) for each voltammogram. \n",
    "    Empty values are assumed as 0 concentration\"\"\"\n",
    "    myY = pd.DataFrame(columns=analyteList)\n",
    "    myY['peak_labels'] = myXlabels\n",
    "    myY.set_index('peak_labels',inplace=True)\n",
    "    for col in myY.columns:\n",
    "        myY[col]=myXlabels\n",
    "    \n",
    "    for col in myY.columns:\n",
    "        myY[col]=myY.index.map(mySchema[col])\n",
    "\n",
    "    if fill_null == True:\n",
    "        myY.fillna(0,inplace=True) \n",
    "        #empty values will be assumed as 0; dangerous if samples are mislabeled!\n",
    "    \n",
    "    return myY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(myFile,analyteList,mySamples='Signals',myConcentrations='PeaksLabel'): \n",
    "    \"\"\"Takes in file location (myFile) and list of desired analytes (analyteList)\n",
    "    and returns X_train and y_train arrays. Analytes must be named in same manner as data file\"\"\"\n",
    "    \n",
    "    if myFile[-3:] == 'mat':\n",
    "        mat1 = mat73.loadmat(myFile) \n",
    "        myX = mat1.get('Signals')\n",
    "        peak_labels = mat1.get('PeaksLabel') \n",
    "        myX = pd.DataFrame(myX,index=peak_labels)\n",
    "\n",
    "        return myX.T, peak_labels\n",
    "    \n",
    "    if myFile[-4:] == 'xlsx':\n",
    "        myData = pd.read_excel(myFile, sheet_name='Sheet1') \n",
    "        peak_labels=myData.columns.to_list()\n",
    "        myX=myData\n",
    "        \n",
    "        return myX, peak_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_delimiter(myData,myDelimiter='.'):\n",
    "    \"\"\"given data (myData) as a list or pandas dataframe, \n",
    "    return data with delimiter (myDelimiter, default is a period) removed\"\"\"\n",
    "    if isinstance(myData, pd.DataFrame):\n",
    "        col_no_decimal = myData.columns.str.split(myDelimiter).str[0]\n",
    "        for i in range(len(myData.columns)):\n",
    "            col = str(myData.columns[i])\n",
    "            if myDelimiter in col:\n",
    "                myData.rename(columns={col:col_no_decimal[i]},inplace=True)\n",
    "                \n",
    "    if isinstance(myData, list):\n",
    "        for i in range(len(myData)):\n",
    "            if myDelimiter in str(myData[i]):\n",
    "                myData[i] = str(myData[i]).split('.')[0]\n",
    "            else:\n",
    "                myData[i] = str(myData[i])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove duplicates\n",
    "def plot_raw_voltammograms(myData):\n",
    "    \"\"\"plots raw data\"\"\"\n",
    "    plt.plot(myData)\n",
    "    #legend_without_duplicate_labels(plt.gca)\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.xlabel(\"Sample\");\n",
    "    plt.ylabel(\"Current (nA)\");\n",
    "    plt.title(\"Overlaid Voltammograms\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preprocessers(myData):\n",
    "    \"\"\"Return plots of preprocessed data\"\"\"\n",
    "    ##TODO:add subplots\n",
    "    # eliminate offset by mean-centering each variable j in a sample i (subtracts mean of j across all i)\n",
    "    # normalize each sample to maintain signal shape (for identification)\n",
    "    # standardize each variable to change signal shape (for concentration)\n",
    "\n",
    "    plt.plot(scale(myData.T).T);\n",
    "    plt.title(\"Standardized Features\");\n",
    "    plt.xlabel(\"Sample number\");\n",
    "    plt.show();\n",
    "\n",
    "    for myNorm in ['l2','l1','max']:\n",
    "        plt.plot(normalize(myData.T,norm=myNorm).T);\n",
    "        plt.xlabel(\"Sample number\");\n",
    "        plt.title(myNorm + \" Normalized Samples\");\n",
    "        plt.show();\n",
    "  \n",
    "    plt.plot((np.diff(myData.T)).T);\n",
    "    plt.xlabel(\"Sample number\");\n",
    "    plt.title(\"Derivative\");\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA(myData,myY,analyteList,xDim = 0, yDim = 1, zDim = None):\n",
    "    \"\"\"plots 2d PCA plots of various preprocessed voltammograms colored by concentration of analytes\"\"\"\n",
    "    pca = PCA(n_components = 2) #for visualization\n",
    "    label_to_color = {}\n",
    "    viridis = cm.get_cmap('viridis', 8)\n",
    "    for i in myY.index.unique():\n",
    "        label_to_color[i]=np.unique(np.sum(myY,axis=1)[i])/np.max(np.sum(myY,axis=1))\n",
    "        \n",
    "    for preProcess in ['Standardized Features','Normalized Samples','Differentiated Samples']:\n",
    "       \n",
    "        if preProcess == 'Differentiated Samples':\n",
    "            principalComponents = pca.fit_transform(np.diff(myData.T))\n",
    "            \n",
    "        if preProcess == 'Standardized Features':\n",
    "            principalComponents = pca.fit_transform(scale(myData.T)) \n",
    "            \n",
    "        if preProcess == 'Normalized Samples':\n",
    "            principalComponents = pca.fit_transform(normalize(myData.T)) \n",
    "        \n",
    "        #plot and label the data in PC space\n",
    "        plt.figure(figsize=(10,10))\n",
    "        k=[]\n",
    "        for i in range(len(myData.columns)):\n",
    "            x_coord=principalComponents[i,xDim]\n",
    "            y_coord=principalComponents[i,yDim]\n",
    "            j=myData.columns[i]\n",
    "            plt.scatter(x_coord, y_coord,color=viridis(label_to_color[j][0]));\n",
    "            if j not in k:\n",
    "                plt.annotate(j,(x_coord,y_coord))\n",
    "                k.append(j)\n",
    "                \n",
    "        #plot origin axes\n",
    "        axes=plt.gca()  \n",
    "        plt.plot([axes.get_xlim()[0],0,axes.get_xlim()[1]],[0,0,0],c='k')\n",
    "        plt.plot([0,0,0],[axes.get_ylim()[0],0,axes.get_ylim()[1]],c='k')\n",
    "        plt.title(str(preProcess) + \" PCs in 2D\")\n",
    "        plt.xlabel(\"PC1\" + ' (' + str(round(pca.explained_variance_ratio_[0]*100,2)) + '%)' )\n",
    "        plt.ylabel(\"PC2\" + ' (' + str(round(pca.explained_variance_ratio_[1]*100,2)) + '%)' )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legend_without_duplicate_labels(ax):\n",
    "    \"\"\"removes duplicate labels from plot legend\"\"\"\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##in development##\n",
    "#def plot_outliers(myX,nComps):\n",
    "#    \"\"\"use pca package to help plot outliers\n",
    "#    see: https://pypi.org/project/pca/\"\"\"\n",
    "#    myModel = pca(n_components=nComps, alpha=0.05, normalize=True, detect_outliers=['ht2', 'spe'])\n",
    "#    out = myModel.fit_transform(myX)\n",
    "#    model.biplot(legend=True, SPE=True, hotellingt2=True)     \n",
    "#    myXoutliers = myX[results['outliers']['y_bool'],:]\n",
    "#    myXnormal = myX[~results['outliers']['y_bool'],:]\n",
    "#    return myXoutliers, myXnormal\n",
    "#TODO: require updating y as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##in development##\n",
    "#def remove_outliers(removeIndex=[],removeColumns=[],search_mode='manual'):   \n",
    "#    if search_mode == 'SPE/ht2':\n",
    "#        X_train = Xnormal\n",
    "#    if search_mode == 'manual':\n",
    "#        try:\n",
    "#            y_train = y_train.drop(index=removeIndex)\n",
    "#            X_train = X_train.drop(columns=removeColumns)\n",
    "#        except:\n",
    "#            print(\"Could not find samples listed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_preprocess(preProcess,myData,myNorm='max'):\n",
    "    \"\"\"set a pre-processer\"\"\"\n",
    "    if preProcess == 'Scale Features':\n",
    "        preProcesser = preprocessing.StandardScaler().fit(myData) \n",
    "\n",
    "    if preProcess == 'Normalize Samples':\n",
    "        if myNorm == 'l1': \n",
    "            preProcesser = preprocessing.Normalizer(norm=myNorm).fit(myData) \n",
    "        if myNorm == 'l2': \n",
    "            preProcesser = preprocessing.Normalizer(norm=myNorm).fit(myData) \n",
    "        if myNorm == 'max': \n",
    "            preProcesser = preprocessing.Normalizer(norm=myNorm).fit(myData) \n",
    "\n",
    "    if preProcess == 'No Scale':\n",
    "        preProcesser = preprocessing.StandardScaler(with_mean=False,with_std=False).fit(myData) \n",
    "\n",
    "    if preProcess == 'Derivative': \n",
    "        preProcesser = preprocessing.StandardScaler(with_mean=False,with_std=False).fit(np.diff(myData))\n",
    "         \n",
    "    return preProcesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(myModel,myFile):\n",
    "    pickle.dump(myModel, open(myFile, 'wb'))\n",
    "    print('Saved model successfully!')\n",
    "    \n",
    "def load_model(myFile):\n",
    "    myModel = pickle.load(myFile)\n",
    "    return myModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_R2Y(myComp,myX,myY,analyteList,modelChoice,myPreProcess,myCmap):\n",
    "    \"\"\"fits a model (PCR or PLSR) to data using the set pre-processer and plots explained variance by analyte \n",
    "    for starting from 1 component model up to the number of components (myComp) desired\"\"\"\n",
    "    if modelChoice == 'PLSR' or modelChoice == \"PCR\":\n",
    "        \n",
    "        variance_explained = pd.DataFrame(columns=['N']+analyteList)\n",
    "        \n",
    "        i_values = np.arange(1,myComp+1)\n",
    "        \n",
    "        for i in i_values:\n",
    "            \n",
    "            variance_explained.loc[i-1,'N'] = i\n",
    "            \n",
    "            if modelChoice == 'PLSR':\n",
    "                myModel = make_pipeline(myPreProcess, PLSRegression(n_components=i,scale=False))\n",
    "                myModel.fit_transform(myX.T, myY)\n",
    "\n",
    "            if modelChoice == 'PCR':\n",
    "                myModel = make_pipeline(myPreProcess, PCA(n_components=i), LinearRegression()) \n",
    "                myModel.fit(myX.T, myY)\n",
    "\n",
    "            y_pred = myModel.predict(myX.T)\n",
    "\n",
    "            for j in range(len(analyteList)):\n",
    "                idx=analyteList[j]\n",
    "                variance_explained.loc[i-1,idx] = r2_score(myY,y_pred,multioutput='raw_values')[j]\n",
    "                \n",
    "    print(variance_explained)\n",
    "\n",
    "    for analyte in analyteList:\n",
    "        plt.plot(i_values,variance_explained[analyte],linewidth=3,color=myCmap[analyte],label=str(analyte+' R2Y'),linestyle='dotted');\n",
    "\n",
    "    plt.legend();\n",
    "    plt.title(\"Variance Explained by N Components\");\n",
    "    plt.xlabel(\"Number Components\");\n",
    "    plt.ylabel(\"Variance Explained\");\n",
    "    #TODO:plot average in black?\n",
    "    \n",
    "    return y_pred, variance_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q2Y(myComp,myX,myY,analyteList,modelChoice,myPreProcess,myCmap,myCV=5):\n",
    "    #TODO: update CV strategy?\n",
    "    \"\"\"cross validated version of get_R2Y. Set cross validation folders using myCV. Default is 5-fold.\"\"\"\n",
    "    if modelChoice == 'PLSR' or modelChoice == \"PCR\":\n",
    "        \n",
    "        variance_explained_CV = pd.DataFrame(columns=['N']+analyteList)\n",
    "        \n",
    "        i_values = np.arange(1,myComp+1)\n",
    "\n",
    "        for i in i_values:\n",
    "            \n",
    "            variance_explained_CV.loc[i-1,'N'] = i\n",
    "            \n",
    "            if modelChoice == 'PLSR':\n",
    "                myModel = make_pipeline(myPreProcess, PLSRegression(n_components=i,scale=False))\n",
    "                myModel.fit_transform(myX.T, myY)\n",
    "\n",
    "            if modelChoice == 'PCR':\n",
    "                myModel = make_pipeline(myPreProcess, PCA(n_components=i), LinearRegression()) \n",
    "                myModel.fit(myX.T, myY) \n",
    "\n",
    "            y_pred_CV = cross_val_predict(myModel, myX.T, myY, cv=myCV)\n",
    "\n",
    "            for j in range(len(analyteList)):\n",
    "                idx=analyteList[j]\n",
    "                variance_explained_CV.loc[i-1,idx] = r2_score(myY,y_pred_CV,multioutput='raw_values')[j]\n",
    "\n",
    "    print(variance_explained_CV)\n",
    "\n",
    "    for analyte in analyteList:\n",
    "        plt.plot(i_values,variance_explained_CV[analyte],linewidth=3,color=myCmap[analyte],label=str(analyte+' Q2Y'));\n",
    "\n",
    "    plt.legend();\n",
    "    plt.title(\"Variance Explained by N Components during \"+str(myCV)+\" Fold CV\");\n",
    "    plt.xlabel(\"Number Components\");\n",
    "    plt.ylabel(\"Variance Explained\");\n",
    "    plt.show();\n",
    "    \n",
    "    return y_pred_CV, variance_explained_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(modelChoice,myX, myY,myComp,preProcesser):\n",
    "    \"\"\"sets and re-trains model on all data\"\"\"\n",
    "    if modelChoice == 'PLSR':\n",
    "        myModel = make_pipeline(preProcesser, PLSRegression(n_components=myComp,scale=False))\n",
    "        myModel.fit_transform(myX, myY)\n",
    "\n",
    "    if modelChoice == 'PCR':\n",
    "        myModel = make_pipeline(preProcesser, PCA(n_components=myComp), LinearRegression()) \n",
    "        myModel.fit(myX, myY)\n",
    "    \n",
    "    return myModel\n",
    "    #TODO:add elastic net, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: incorporate pipeline\n",
    "def reconstruct_voltammograms(myX,modelChoice,nComponents):\n",
    "    if modelChoice == 'PCR':\n",
    "        pca = PCA(n_components=nComponents);\n",
    "        myX_reduced = pca.fit_transform(myX);\n",
    "        myX_recovered = pca.inverse_transform(myX_reduced);\n",
    "\n",
    "    if modelChoice == 'PLSR':\n",
    "        plsr = PLSRegression(n_components=nComponents,scale=False);\n",
    "        myX_reduced = plsr.fit_transform(myX,y);\n",
    "        myX_recovered = plsr.inverse_transform(plsr.x_scores_);\n",
    "\n",
    "    plt.plot(myX.T);\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Preprocessed Current (nA)')\n",
    "    plt.title('Preprocessed Voltammogram')\n",
    "    plt.show();\n",
    "    plt.plot((myX_recovered).T);\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Preprocessed Current (nA)')\n",
    "    plt.title('Reconstructed Preprocessed Voltammogram')\n",
    "    plt.show()\n",
    "\n",
    "    if preProcess != 'Normalize Samples':\n",
    "        plt.plot((preProcesser.inverse_transform(myX)).T);\n",
    "        plt.xlabel('Sample')\n",
    "        plt.ylabel('Current (nA)')\n",
    "        plt.title('Voltammogram')\n",
    "        plt.show();\n",
    "        plt.plot((preProcesser.inverse_transform(myX_recovered)).T);\n",
    "        plt.xlabel('Sample')\n",
    "        plt.ylabel('Current (nA)')\n",
    "        plt.title('Reconstructed Voltammogram')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Reconstruction Error (%):\", 100*round(1-r2_score(myX,myX_recovered),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_curves(myModel,myX,myY,analyteList,myCmap):\n",
    "    \"\"\"plot predicted versus actual concentrations and r2-values\"\"\"\n",
    "    myY_pred = myModel.predict(myX)\n",
    "    \n",
    "    myY_pred=pd.DataFrame(myY_pred,columns=[i for i in analyteList])\n",
    "\n",
    "    r2_scores = pd.DataFrame(columns=analyteList)\n",
    "    \n",
    "    for i in range(len(analyteList)):\n",
    "        r2_scores.loc[i,analyteList[i]] = r2_score(myY[analyteList[i]],myY_pred[analyteList[i]],multioutput='raw_values')\n",
    "        \n",
    "    for i in range(len(analyteList)):\n",
    "        plt.scatter(myY[analyteList[i]],myY_pred[analyteList[i]], label=analyteList[i],color=myCmap[analyteList[i]]);\n",
    "        \n",
    "    plt.xlabel('Observed Concentration')\n",
    "    plt.ylabel('Predicted Concentration')\n",
    "    plt.title(\"Predicted versus Observed Concentration\")\n",
    "    abline(1,0)\n",
    "    plt.plot();\n",
    "    plt.legend();\n",
    "    print(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vip(myModel):\n",
    "    \"\"\"calculate and plot vip scores from set PLSR model. \n",
    "    Adapted from: https://github.com/scikit-learn/scikit-learn/issues/7050\"\"\"\n",
    "    myModel = myModel.named_steps['plsregression']\n",
    "    t = myModel.x_scores_\n",
    "    w = myModel.x_weights_\n",
    "    q = myModel.y_loadings_\n",
    "    p, h = w.shape\n",
    "    vips = np.zeros((p,))\n",
    "    s = np.diag(t.T @ t @ q.T @ q).reshape(h, -1)\n",
    "    total_s = np.sum(s)\n",
    "    for i in range(p):\n",
    "        weight = np.array([ (w[i,j] / np.linalg.norm(w[:,j]))**2 for j in range(h) ])\n",
    "        vips[i] = np.sqrt(p*(s.T @ weight)/total_s)\n",
    "    \n",
    "    plt.plot(vips);\n",
    "    plt.xlabel('Sampled Point')\n",
    "    plt.ylabel('VIP Score')\n",
    "    return vips;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n) :\n",
    "    \"\"\"moving average of data a for given interval n\"\"\"\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept):\n",
    "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(myX,n):\n",
    "    \"\"\"takes every nth sample\"\"\"\n",
    "    myXds=myX.iloc[::n, :]\n",
    "    myXds.reset_index(drop=True,inplace=True)\n",
    "    plt.plot(myX);\n",
    "    plt.title('Original Data')\n",
    "    plt.xlabel('Sampled Point')\n",
    "    plt.ylabel('Current (nA)')\n",
    "    plt.show()\n",
    "    plt.plot(myXds);\n",
    "    plt.xlabel('Sampled Point')\n",
    "    plt.ylabel('Current (nA)')\n",
    "    plt.title('Downsampled Data')\n",
    "    plt.show()\n",
    "    return myXds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexes(dfObj, value):\n",
    "    \"\"\"look for given value in dataframe dfObj and return its index\"\"\"\n",
    "    # Empty list\n",
    "    listOfPos = []\n",
    "     \n",
    "    # isin() method will return a dataframe with\n",
    "    # boolean values, True at the positions   \n",
    "    # where element exists\n",
    "    result = dfObj.isin([value])\n",
    "     \n",
    "    # any() method will return\n",
    "    # a boolean series\n",
    "    seriesObj = result.any()\n",
    " \n",
    "    # Get list of column names where\n",
    "    # element exists\n",
    "    columnNames = list(seriesObj[seriesObj == True].index)\n",
    "    \n",
    "    # Iterate over the list of columns and\n",
    "    # extract the row index where element exists\n",
    "    for col in columnNames:\n",
    "        rows = list(result[col][result[col] == True].index)\n",
    " \n",
    "        for row in rows:\n",
    "            listOfPos.append((row, col))\n",
    "             \n",
    "    # This list contains a list tuples with\n",
    "    # the index of element in the dataframe\n",
    "    return listOfPos\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
